{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-26T11:36:09.539187Z",
     "iopub.status.busy": "2024-03-26T11:36:09.538167Z",
     "iopub.status.idle": "2024-03-26T11:36:23.840443Z",
     "shell.execute_reply": "2024-03-26T11:36:23.839444Z",
     "shell.execute_reply.started": "2024-03-26T11:36:09.539149Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import nltk\n",
    "import torch\n",
    "import spacy\n",
    "import getpass\n",
    "import warnings\n",
    "import transformers\n",
    "from nltk.corpus import stopwords\n",
    "from langchain import PromptTemplate\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, pipeline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "transformers.logging.set_verbosity_warning()\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "Lemmatizer = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up and Using the Hugging Face API for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T11:24:20.801527Z",
     "iopub.status.busy": "2024-03-26T11:24:20.800918Z",
     "iopub.status.idle": "2024-03-26T11:25:02.032422Z",
     "shell.execute_reply": "2024-03-26T11:25:02.031667Z",
     "shell.execute_reply.started": "2024-03-26T11:24:20.801501Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place your token here:  ·····································\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Place your token here: \")\n",
    "Template = \"\"\"given the Topic below, generate text about this topic in details.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "Topic:\n",
    "{input}\n",
    "\n",
    "Response: \"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "                        template=Template,\n",
    "                        input_variables=[\"history\", \"input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Configuring the Vicuna Model for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T11:25:02.033770Z",
     "iopub.status.busy": "2024-03-26T11:25:02.033474Z",
     "iopub.status.idle": "2024-03-26T11:29:26.917995Z",
     "shell.execute_reply": "2024-03-26T11:29:26.916850Z",
     "shell.execute_reply.started": "2024-03-26T11:25:02.033746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    model_id = \"/kaggle/input/vicuna/pytorch/7b-v1pt5/1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, temperature=1.0, torch_dtype = torch.float16, device_map = \"auto\", offload_folder = \"./cache\")\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512,streamer=streamer, return_full_text=False)\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Conversation Chain for Text Generation with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T11:29:26.920848Z",
     "iopub.status.busy": "2024-03-26T11:29:26.920509Z",
     "iopub.status.idle": "2024-03-26T11:29:26.926607Z",
     "shell.execute_reply": "2024-03-26T11:29:26.925652Z",
     "shell.execute_reply.started": "2024-03-26T11:29:26.920820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT,\n",
    "    verbose=False, \n",
    "    memory=ConversationBufferMemory(ai_prefix=\"Response\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Three Different Topic Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T11:29:26.927897Z",
     "iopub.status.busy": "2024-03-26T11:29:26.927606Z",
     "iopub.status.idle": "2024-03-26T11:30:52.980966Z",
     "shell.execute_reply": "2024-03-26T11:30:52.979868Z",
     "shell.execute_reply.started": "2024-03-26T11:29:26.927873Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I love cars. They are a symbol of freedom and independence. They come in all shapes and sizes, from sleek sports cars to rugged off-road vehicles. Some people collect cars as a hobby, while others use them for practical purposes like transportation or work. Cars have evolved over the years, with new technologies and features being added all the time. For example, today's cars are more fuel-efficient and environmentally friendly than ever before. They also have advanced safety features like lane departure warning and automatic emergency braking. As someone who loves cars, I am always excited to see what new innovations will be introduced in the future.</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shopping is a fun and exciting experience for many people. It allows us to explore new products and brands, and to find items that we need or want for our lives. Whether we are shopping in person or online, there are endless options available to us. From clothing and accessories to electronics and home goods, the possibilities are endless. Some people enjoy shopping as a leisure activity, while others use it as a practical way to get the things they need. No matter what our reasons for shopping are, it is always a great way to treat ourselves or to find unique gifts for others.</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Anime is a genre of Japanese animation that has gained popularity around the world. It is known for its unique style, which often features exaggerated facial expressions, colorful graphics, and fantastical themes. Many anime series are based on manga, or Japanese comics, and often explore themes of friendship, love, and personal growth. Some of the most popular anime series include \"Naruto,\" \"Dragon Ball Z,\" and \"One Piece.\" In addition to television shows and movies, anime has also become popular in other forms of media, such as video games and merchandise. As someone who loves anime, I am always excited to see what new series or movies will be released in the future.</s>\n"
     ]
    }
   ],
   "source": [
    "doc1 = generation(\"cars\")['response']\n",
    "doc2 = generation(\"shopping\")['response']\n",
    "doc3 = generation(\"anime\")['response']\n",
    "Docs = [doc1, doc2, doc3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T12:51:07.966466Z",
     "iopub.status.busy": "2024-03-26T12:51:07.965760Z",
     "iopub.status.idle": "2024-03-26T12:51:07.976565Z",
     "shell.execute_reply": "2024-03-26T12:51:07.975492Z",
     "shell.execute_reply.started": "2024-03-26T12:51:07.966432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def Clean_Text(Text):\n",
    "    Cleaned_Text = re.sub(r'[^\\w\\s]', '', Text)\n",
    "    Cleaned_Text = re.sub(r'\\d+', '', Cleaned_Text)\n",
    "    Cleaned_Text = Cleaned_Text.lower()\n",
    "    return Cleaned_Text\n",
    "\n",
    "def Tokenize_Text(Text):\n",
    "    tokens = word_tokenize(Text)\n",
    "    return tokens\n",
    "\n",
    "def Lemmatize_Text(tokens):\n",
    "    Lemmatized_Tokens = []\n",
    "    for token in tokens:\n",
    "        if Lemmatizer(token)[0].lemma_ != '-PRON-':\n",
    "            Lemmatized_Token = Lemmatizer(token)[0].lemma_\n",
    "        else:\n",
    "            Lemmatized_Token = token\n",
    "        Lemmatized_Tokens.append(Lemmatized_Token)\n",
    "    return Lemmatized_Tokens\n",
    "\n",
    "def Remove_Stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_stop_words = ['I', 'z']\n",
    "    stop_words.update(custom_stop_words)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "def Get_Unique_Words(text):\n",
    "    Unique_Words = set(text)\n",
    "    Sorted_Words = sorted(Unique_Words)\n",
    "    return ' '.join(Sorted_Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the Generated Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T12:51:18.899898Z",
     "iopub.status.busy": "2024-03-26T12:51:18.899068Z",
     "iopub.status.idle": "2024-03-26T12:51:22.421800Z",
     "shell.execute_reply": "2024-03-26T12:51:22.420477Z",
     "shell.execute_reply.started": "2024-03-26T12:51:18.899856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Filtered_Docs = []\n",
    "for i in range (len(Docs)):\n",
    "    Cleaned_Text = Clean_Text(Docs[i])\n",
    "    Tokens = Tokenize_Text(Cleaned_Text)\n",
    "    Lemmatized_Tokens = Lemmatize_Text(Tokens)\n",
    "    Filtered_Tokens = Remove_Stopwords(Lemmatized_Tokens)\n",
    "    Unique_Words = Get_Unique_Words(Filtered_Tokens)\n",
    "    Unique_Words = Unique_Words.split('\\n')\n",
    "    Filtered_Unique = [word for word in Unique_Words if len(word) > 2] # Remove words with less than 2 characters\n",
    "    Filtered_Text = ' '.join(Filtered_Unique)\n",
    "    Filtered_Docs.append(Filtered_Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the TF-IDF for Each Document Using Built-in TfidfVectorizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T12:08:54.300439Z",
     "iopub.status.busy": "2024-03-26T12:08:54.299644Z",
     "iopub.status.idle": "2024-03-26T12:08:54.313934Z",
     "shell.execute_reply": "2024-03-26T12:08:54.312705Z",
     "shell.execute_reply.started": "2024-03-26T12:08:54.300404Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF for Document 1:\n",
      "add: 0.1483\n",
      "advance: 0.1483\n",
      "also: 0.1128\n",
      "always: 0.0876\n",
      "automatic: 0.1483\n",
      "brake: 0.1483\n",
      "car: 0.1483\n",
      "collect: 0.1483\n",
      "come: 0.1483\n",
      "departure: 0.1483\n",
      "emergency: 0.1483\n",
      "environmentally: 0.1483\n",
      "ever: 0.1483\n",
      "evolve: 0.1483\n",
      "example: 0.1483\n",
      "excite: 0.1128\n",
      "feature: 0.1128\n",
      "freedom: 0.1483\n",
      "friendly: 0.1483\n",
      "fuelefficient: 0.1483\n",
      "future: 0.1128\n",
      "hobby: 0.1483\n",
      "independence: 0.1483\n",
      "innovation: 0.1483\n",
      "introduce: 0.1483\n",
      "lane: 0.1483\n",
      "like: 0.1483\n",
      "love: 0.1128\n",
      "new: 0.0876\n",
      "offroad: 0.1483\n",
      "people: 0.1128\n",
      "practical: 0.1128\n",
      "purpose: 0.1483\n",
      "rugged: 0.1483\n",
      "safety: 0.1483\n",
      "see: 0.1128\n",
      "shape: 0.1483\n",
      "size: 0.1483\n",
      "sleek: 0.1483\n",
      "someone: 0.1128\n",
      "sport: 0.1483\n",
      "symbol: 0.1483\n",
      "technology: 0.1483\n",
      "time: 0.1483\n",
      "today: 0.1483\n",
      "transportation: 0.1483\n",
      "use: 0.1128\n",
      "vehicle: 0.1483\n",
      "warn: 0.1483\n",
      "work: 0.1483\n",
      "year: 0.1483\n",
      "\n",
      "TF-IDF for Document 2:\n",
      "accessory: 0.1598\n",
      "activity: 0.1598\n",
      "allow: 0.1598\n",
      "always: 0.0944\n",
      "available: 0.1598\n",
      "brand: 0.1598\n",
      "clothing: 0.1598\n",
      "electronic: 0.1598\n",
      "endless: 0.1598\n",
      "enjoy: 0.1598\n",
      "exciting: 0.1598\n",
      "experience: 0.1598\n",
      "explore: 0.1215\n",
      "find: 0.1598\n",
      "fun: 0.1598\n",
      "get: 0.1598\n",
      "gift: 0.1598\n",
      "good: 0.1598\n",
      "great: 0.1598\n",
      "home: 0.1598\n",
      "item: 0.1598\n",
      "leisure: 0.1598\n",
      "live: 0.1598\n",
      "many: 0.1215\n",
      "matter: 0.1598\n",
      "need: 0.1598\n",
      "new: 0.0944\n",
      "online: 0.1598\n",
      "option: 0.1598\n",
      "people: 0.1215\n",
      "person: 0.1598\n",
      "possibility: 0.1598\n",
      "practical: 0.1215\n",
      "product: 0.1598\n",
      "reason: 0.1598\n",
      "shopping: 0.1598\n",
      "thing: 0.1598\n",
      "treat: 0.1598\n",
      "unique: 0.1215\n",
      "use: 0.1215\n",
      "want: 0.1598\n",
      "way: 0.1598\n",
      "whether: 0.1598\n",
      "\n",
      "TF-IDF for Document 3:\n",
      "addition: 0.1422\n",
      "also: 0.1081\n",
      "always: 0.0840\n",
      "animation: 0.1422\n",
      "anime: 0.1422\n",
      "around: 0.1422\n",
      "ball: 0.1422\n",
      "base: 0.1422\n",
      "become: 0.1422\n",
      "colorful: 0.1422\n",
      "comic_strip: 0.1422\n",
      "dragon: 0.1422\n",
      "exaggerate: 0.1422\n",
      "excite: 0.1081\n",
      "explore: 0.1081\n",
      "expression: 0.1422\n",
      "facial: 0.1422\n",
      "fantastical: 0.1422\n",
      "feature: 0.1081\n",
      "form: 0.1422\n",
      "friendship: 0.1422\n",
      "future: 0.1081\n",
      "gain: 0.1422\n",
      "game: 0.1422\n",
      "genre: 0.1422\n",
      "graphic: 0.1422\n",
      "growth: 0.1422\n",
      "include: 0.1422\n",
      "japanese: 0.1422\n",
      "know: 0.1422\n",
      "love: 0.1081\n",
      "manga: 0.1422\n",
      "many: 0.1081\n",
      "medium: 0.1422\n",
      "merchandise: 0.1422\n",
      "movie: 0.1422\n",
      "naruto: 0.1422\n",
      "new: 0.0840\n",
      "often: 0.1422\n",
      "one: 0.1422\n",
      "personal: 0.1422\n",
      "piece: 0.1422\n",
      "popular: 0.1422\n",
      "popularity: 0.1422\n",
      "release: 0.1422\n",
      "see: 0.1081\n",
      "series: 0.1422\n",
      "show: 0.1422\n",
      "someone: 0.1081\n",
      "style: 0.1422\n",
      "television: 0.1422\n",
      "theme: 0.1422\n",
      "unique: 0.1081\n",
      "video: 0.1422\n",
      "world: 0.1422\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(Filtered_Docs)\n",
    "\n",
    "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i, doc in enumerate(Filtered_Docs):\n",
    "    print(f\"TF-IDF for Document {i+1}:\")\n",
    "    for j, feature in enumerate(feature_names):\n",
    "        tfidf_value = tfidf_matrix_dense[i, j]\n",
    "        if tfidf_value > 0:\n",
    "            print(f\"{feature}: {tfidf_value:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing TF-IDF from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf(documents):\n",
    "    tf_dicts = []\n",
    "    for doc in documents:\n",
    "        tf_dict = {}\n",
    "        for word in doc.split():\n",
    "            tf_dict[word] = tf_dict.get(word, 0) + 1\n",
    "        tf_dicts.append(tf_dict)\n",
    "    return tf_dicts\n",
    "\n",
    "def calculate_idf(documents):\n",
    "    num_documents = len(documents)\n",
    "    idfs = {}\n",
    "    for doc in documents:\n",
    "        for word in doc.split():\n",
    "            idfs[word] = idfs.get(word, 0) + 1\n",
    "\n",
    "    for word, df in idfs.items():\n",
    "        idfs[word] = math.log(1 + num_documents / (1 + df)) + 1 \n",
    "    return idfs\n",
    "\n",
    "def calculate_tfidf(tf_dicts, idfs):\n",
    "    tfidf_matrix = []\n",
    "    for tf_dict in tf_dicts:\n",
    "        tfidf_doc = {}\n",
    "        for word, tf in tf_dict.items():\n",
    "            tfidf_doc[word] = tf * idfs[word]\n",
    "        tfidf_matrix.append(tfidf_doc)\n",
    "    return tfidf_matrix\n",
    "\n",
    "def normalize_tfidf(tfidf_matrix):\n",
    "    for doc_tfidf in tfidf_matrix:\n",
    "        squared_sum = sum(value**2 for value in doc_tfidf.values())\n",
    "        doc_norm = math.sqrt(squared_sum)\n",
    "        for word, tfidf in doc_tfidf.items():\n",
    "            doc_tfidf[word] = tfidf / doc_norm\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating TF-IDF for Each Document Using the Previous Handcrafted Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T12:51:37.054736Z",
     "iopub.status.busy": "2024-03-26T12:51:37.054342Z",
     "iopub.status.idle": "2024-03-26T12:51:37.071628Z",
     "shell.execute_reply": "2024-03-26T12:51:37.070619Z",
     "shell.execute_reply.started": "2024-03-26T12:51:37.054698Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF for Document 1:\n",
      "add: 0.1441\n",
      "advance: 0.1441\n",
      "also: 0.1274\n",
      "always: 0.1173\n",
      "automatic: 0.1441\n",
      "brake: 0.1441\n",
      "car: 0.1441\n",
      "collect: 0.1441\n",
      "come: 0.1441\n",
      "departure: 0.1441\n",
      "emergency: 0.1441\n",
      "environmentally: 0.1441\n",
      "ever: 0.1441\n",
      "evolve: 0.1441\n",
      "example: 0.1441\n",
      "excite: 0.1274\n",
      "feature: 0.1274\n",
      "freedom: 0.1441\n",
      "friendly: 0.1441\n",
      "fuelefficient: 0.1441\n",
      "future: 0.1274\n",
      "hobby: 0.1441\n",
      "independence: 0.1441\n",
      "innovation: 0.1441\n",
      "introduce: 0.1441\n",
      "lane: 0.1441\n",
      "like: 0.1441\n",
      "love: 0.1274\n",
      "new: 0.1173\n",
      "offroad: 0.1441\n",
      "people: 0.1274\n",
      "practical: 0.1274\n",
      "purpose: 0.1441\n",
      "rugged: 0.1441\n",
      "safety: 0.1441\n",
      "see: 0.1274\n",
      "shape: 0.1441\n",
      "size: 0.1441\n",
      "sleek: 0.1441\n",
      "someone: 0.1274\n",
      "sport: 0.1441\n",
      "symbol: 0.1441\n",
      "technology: 0.1441\n",
      "time: 0.1441\n",
      "today: 0.1441\n",
      "transportation: 0.1441\n",
      "use: 0.1274\n",
      "vehicle: 0.1441\n",
      "warn: 0.1441\n",
      "work: 0.1441\n",
      "year: 0.1441\n",
      "\n",
      "TF-IDF for Document 2:\n",
      "accessory: 0.1562\n",
      "activity: 0.1562\n",
      "allow: 0.1562\n",
      "always: 0.1271\n",
      "available: 0.1562\n",
      "brand: 0.1562\n",
      "clothing: 0.1562\n",
      "electronic: 0.1562\n",
      "endless: 0.1562\n",
      "enjoy: 0.1562\n",
      "exciting: 0.1562\n",
      "experience: 0.1562\n",
      "explore: 0.1380\n",
      "find: 0.1562\n",
      "fun: 0.1562\n",
      "get: 0.1562\n",
      "gift: 0.1562\n",
      "good: 0.1562\n",
      "great: 0.1562\n",
      "home: 0.1562\n",
      "item: 0.1562\n",
      "leisure: 0.1562\n",
      "live: 0.1562\n",
      "many: 0.1380\n",
      "matter: 0.1562\n",
      "need: 0.1562\n",
      "new: 0.1271\n",
      "online: 0.1562\n",
      "option: 0.1562\n",
      "people: 0.1380\n",
      "person: 0.1562\n",
      "possibility: 0.1562\n",
      "practical: 0.1380\n",
      "product: 0.1562\n",
      "reason: 0.1562\n",
      "shopping: 0.1562\n",
      "thing: 0.1562\n",
      "treat: 0.1562\n",
      "unique: 0.1380\n",
      "use: 0.1380\n",
      "want: 0.1562\n",
      "way: 0.1562\n",
      "whether: 0.1562\n",
      "\n",
      "TF-IDF for Document 3:\n",
      "addition: 0.1385\n",
      "also: 0.1224\n",
      "always: 0.1127\n",
      "animation: 0.1385\n",
      "anime: 0.1385\n",
      "around: 0.1385\n",
      "ball: 0.1385\n",
      "base: 0.1385\n",
      "become: 0.1385\n",
      "colorful: 0.1385\n",
      "comic_strip: 0.1385\n",
      "dragon: 0.1385\n",
      "exaggerate: 0.1385\n",
      "excite: 0.1224\n",
      "explore: 0.1224\n",
      "expression: 0.1385\n",
      "facial: 0.1385\n",
      "fantastical: 0.1385\n",
      "feature: 0.1224\n",
      "form: 0.1385\n",
      "friendship: 0.1385\n",
      "future: 0.1224\n",
      "gain: 0.1385\n",
      "game: 0.1385\n",
      "genre: 0.1385\n",
      "graphic: 0.1385\n",
      "growth: 0.1385\n",
      "include: 0.1385\n",
      "japanese: 0.1385\n",
      "know: 0.1385\n",
      "love: 0.1224\n",
      "manga: 0.1385\n",
      "many: 0.1224\n",
      "medium: 0.1385\n",
      "merchandise: 0.1385\n",
      "movie: 0.1385\n",
      "naruto: 0.1385\n",
      "new: 0.1127\n",
      "often: 0.1385\n",
      "one: 0.1385\n",
      "personal: 0.1385\n",
      "piece: 0.1385\n",
      "popular: 0.1385\n",
      "popularity: 0.1385\n",
      "release: 0.1385\n",
      "see: 0.1224\n",
      "series: 0.1385\n",
      "show: 0.1385\n",
      "someone: 0.1224\n",
      "style: 0.1385\n",
      "television: 0.1385\n",
      "theme: 0.1385\n",
      "unique: 0.1224\n",
      "video: 0.1385\n",
      "world: 0.1385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_values = calculate_tf(Filtered_Docs)\n",
    "idf_values = calculate_idf(Filtered_Docs)\n",
    "tfidf_matrix = calculate_tfidf(tf_values, idf_values)\n",
    "normalzied_tfidf_matrix = normalize_tfidf(tfidf_matrix)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = []\n",
    "for doc in Filtered_Docs:\n",
    "    feature_names.extend(doc.split())\n",
    "feature_names = list(set(feature_names)) \n",
    "feature_names.sort()\n",
    "\n",
    "# Print TF-IDF results \n",
    "for i, doc in enumerate(Filtered_Docs):\n",
    "    print(f\"TF-IDF for Document {i+1}:\")\n",
    "    for j, feature in enumerate(feature_names):\n",
    "        tfidf_value = tfidf_matrix[i].get(feature, 0)\n",
    "        if tfidf_value > 0:\n",
    "            print(f\"{feature}: {tfidf_value:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 3464,
     "sourceId": 4674,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
