{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4140,"sourceType":"datasetVersion","datasetId":2477}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import re\nimport random\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom gensim.models import Word2Vec\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout, Flatten","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T01:52:21.502813Z","iopub.execute_input":"2025-01-04T01:52:21.503096Z","iopub.status.idle":"2025-01-04T01:52:41.348276Z","shell.execute_reply.started":"2025-01-04T01:52:21.503074Z","shell.execute_reply":"2025-01-04T01:52:41.347347Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Loading and Reading the Dataset","metadata":{}},{"cell_type":"code","source":"# Load the dataset\nfile_path = \"/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\"\n\n# Importing the dataset\nDATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nDATASET_ENCODING = \"ISO-8859-1\"\ndataset = pd.read_csv(file_path,encoding=DATASET_ENCODING , names=DATASET_COLUMNS)\n\n# Removing the unnecessary columns\ndataset = dataset[['sentiment','text']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T01:52:41.349537Z","iopub.execute_input":"2025-01-04T01:52:41.350045Z","iopub.status.idle":"2025-01-04T01:52:46.739827Z","shell.execute_reply.started":"2025-01-04T01:52:41.350021Z","shell.execute_reply":"2025-01-04T01:52:46.739103Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Preprocessing Tweets using Regular Expressions","metadata":{}},{"cell_type":"code","source":"# Defining regex patterns\nurlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|(www\\.)[^ ]*)\"\nuserPattern       = '@[^\\s]+'\nalphaPattern      = \"[^a-z0-9<>]\"\nhashtagPattern    = '#[^\\s]+'\nsequencePattern   = r\"(.)\\1\\1+\"\nseqReplacePattern = r\"\\1\\1\"\n\n# Defining regex for emojis\nsademoji          = r\"[8:=;]['`\\-]?\\(+\"\nlolemoji          = r\"[8:=;]['`\\-]?p+\"\nsmileemoji        = r\"[8:=;]['`\\-]?[)d]+\"\nneutralemoji      = r\"[8:=;]['`\\-]?[\\/|l*]\"\n\ndef apply_preprocessing(tweet):\n\n    tweet = tweet.lower()\n\n    # Replace all URls with '<url>'\n    tweet = re.sub(urlPattern,'<url>',tweet)\n    \n    # Replace @USERNAME to '<user>'\n    tweet = re.sub(userPattern,'<user>', tweet)\n\n    # Replace 3 or more consecutive letters by 2 letters\n    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n\n    # Replace all emojis.\n    tweet = re.sub(r'<3', '<heart>', tweet)\n    tweet = re.sub(smileemoji, '<smile>', tweet)\n    tweet = re.sub(sademoji, '<sadface>', tweet)\n    tweet = re.sub(neutralemoji, '<neutralface>', tweet)\n    tweet = re.sub(lolemoji, '<lolface>', tweet)\n\n    # Remove non-alphanumeric and symbols\n    tweet = re.sub(alphaPattern, ' ', tweet)\n\n    # Adding space on either side of '/' to seperate words (After replacing URLS)\n    tweet = re.sub(r'/', ' / ', tweet)\n    return tweet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T01:52:46.741689Z","iopub.execute_input":"2025-01-04T01:52:46.741994Z","iopub.status.idle":"2025-01-04T01:52:46.747530Z","shell.execute_reply.started":"2025-01-04T01:52:46.741965Z","shell.execute_reply":"2025-01-04T01:52:46.746839Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Apply Preprocessing on the Text Column","metadata":{}},{"cell_type":"code","source":"dataset['processed_text'] = dataset.text.apply(apply_preprocessing)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T01:52:46.748637Z","iopub.execute_input":"2025-01-04T01:52:46.748903Z","iopub.status.idle":"2025-01-04T01:53:16.646731Z","shell.execute_reply.started":"2025-01-04T01:52:46.748884Z","shell.execute_reply":"2025-01-04T01:53:16.645981Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Extracting and Splitting the Preprocessed Data","metadata":{}},{"cell_type":"code","source":"# Extracting the text and it's sentiment\nX_data, y_data = np.array(dataset['processed_text']), np.array(dataset['sentiment'])\n\n# Splitting the dataset into training and testing\nX_train, X_test, Y_train, Y_test = train_test_split(X_data, y_data,test_size = 0.05, random_state = 42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T01:53:16.647585Z","iopub.execute_input":"2025-01-04T01:53:16.647818Z","iopub.status.idle":"2025-01-04T01:53:16.863889Z","shell.execute_reply.started":"2025-01-04T01:53:16.647799Z","shell.execute_reply":"2025-01-04T01:53:16.863186Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Replacing Labels for Binary Classification Purpose","metadata":{}},{"cell_type":"code","source":"# Replace label value 4 with 1\nY_train[Y_train == 4] = 1\nY_test[Y_test == 4] = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T01:53:16.864669Z","iopub.execute_input":"2025-01-04T01:53:16.864953Z","iopub.status.idle":"2025-01-04T01:53:16.879152Z","shell.execute_reply.started":"2025-01-04T01:53:16.864926Z","shell.execute_reply":"2025-01-04T01:53:16.878453Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Converting the Labels to a One Hot Encoding Vector","metadata":{}},{"cell_type":"code","source":"y_train_one_hot = to_categorical(Y_train, num_classes=2)\ny_test_one_hot = to_categorical(Y_test, num_classes=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T01:53:16.880011Z","iopub.execute_input":"2025-01-04T01:53:16.880313Z","iopub.status.idle":"2025-01-04T01:53:16.917261Z","shell.execute_reply.started":"2025-01-04T01:53:16.880283Z","shell.execute_reply":"2025-01-04T01:53:16.916631Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Training a Word2Vec Model to Generate Word Embeddings","metadata":{}},{"cell_type":"code","source":"# Creating Word2Vec training dataset by splitting the training sentences into words\nWord2vec_train_data = [tweet.split() for tweet in X_train]\n\n# Defining the model and training it\nword2vec_model = Word2Vec(Word2vec_train_data, vector_size=100, workers=8, min_count=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T01:53:16.917966Z","iopub.execute_input":"2025-01-04T01:53:16.918194Z","iopub.status.idle":"2025-01-04T01:54:34.076448Z","shell.execute_reply.started":"2025-01-04T01:53:16.918175Z","shell.execute_reply":"2025-01-04T01:54:34.075734Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Initializing the tokenizer with specific settings\ntokenizer = Tokenizer(filters=\"\", lower=False, oov_token=\"<oov>\")\n\n# Fitting the tokenizer on the training data\ntokenizer.fit_on_texts(X_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T01:54:34.078339Z","iopub.execute_input":"2025-01-04T01:54:34.078558Z","iopub.status.idle":"2025-01-04T01:54:52.298727Z","shell.execute_reply.started":"2025-01-04T01:54:34.078538Z","shell.execute_reply":"2025-01-04T01:54:52.297972Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Calculate the maximum length of tokenized documents\ninput_length = max(len(tweet.split()) for tweet in X_data)\n\n# Padding the sequences to be the same length\nX_train_padded = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=input_length)\nX_test_padded  = pad_sequences(tokenizer.texts_to_sequences(X_test) , maxlen=input_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T01:54:52.299716Z","iopub.execute_input":"2025-01-04T01:54:52.299921Z","iopub.status.idle":"2025-01-04T01:55:12.161340Z","shell.execute_reply.started":"2025-01-04T01:54:52.299903Z","shell.execute_reply":"2025-01-04T01:55:12.160647Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Creating an Embedding Matrix from the Word2Vec Model","metadata":{}},{"cell_type":"code","source":"# Defining the vocabulary size\nvocab_length = len(tokenizer.word_index) + 1\n\n# Initializing the embedding matrix\nembedding_matrix = np.zeros((vocab_length, 100))\n\nfor word, token in tokenizer.word_index.items():\n    if word in word2vec_model.wv:\n        embedding_matrix[token] = word2vec_model.wv[word]\n        \nprint(\"Embedding Matrix Shape:\", embedding_matrix.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T01:55:12.162272Z","iopub.execute_input":"2025-01-04T01:55:12.162497Z","iopub.status.idle":"2025-01-04T01:55:12.484710Z","shell.execute_reply.started":"2025-01-04T01:55:12.162478Z","shell.execute_reply":"2025-01-04T01:55:12.483903Z"}},"outputs":[{"name":"stdout","text":"Embedding Matrix Shape: (263128, 100)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Creating and Training a CNN Model for Classification","metadata":{}},{"cell_type":"code","source":"# Define the CNN model\nmodel = Sequential()\n\n# Define the embedding layer with pre-trained weights\nembedding_layer = tf.keras.layers.Embedding(input_dim=vocab_length, output_dim=100, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), trainable=False)\n\n# Add the embedding layer to the model\nmodel.add(embedding_layer)\n\n# Add 1D convolutional layer \nmodel.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n\n# Add global max pooling layer\nmodel.add(GlobalMaxPooling1D())\n\n# Add fully connected layers\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))  # Adding dropout for regularization\nmodel.add(Dense(2, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(X_train_padded, y_train_one_hot, epochs=5, batch_size=64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T01:55:12.485492Z","iopub.execute_input":"2025-01-04T01:55:12.485822Z","iopub.status.idle":"2025-01-04T01:59:02.580071Z","shell.execute_reply.started":"2025-01-04T01:55:12.485792Z","shell.execute_reply":"2025-01-04T01:59:02.579050Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n\u001b[1m23750/23750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 2ms/step - accuracy: 0.7781 - loss: 0.4705\nEpoch 2/5\n\u001b[1m23750/23750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2ms/step - accuracy: 0.8092 - loss: 0.4205\nEpoch 3/5\n\u001b[1m23750/23750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.8145 - loss: 0.4113\nEpoch 4/5\n\u001b[1m23750/23750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.8173 - loss: 0.4060\nEpoch 5/5\n\u001b[1m23750/23750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.8194 - loss: 0.4019\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Evaluate the Model","metadata":{}},{"cell_type":"code","source":"_, accuracy = model.evaluate(X_test_padded, y_test_one_hot)\nprint(\"Testing Accuracy:\", accuracy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T01:59:02.581363Z","iopub.execute_input":"2025-01-04T01:59:02.581709Z","iopub.status.idle":"2025-01-04T01:59:06.269549Z","shell.execute_reply.started":"2025-01-04T01:59:02.581677Z","shell.execute_reply":"2025-01-04T01:59:06.268886Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.8193 - loss: 0.3970\nTesting Accuracy: 0.8190000057220459\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Generate Predictions","metadata":{}},{"cell_type":"code","source":"predictions = model.predict(X_test_padded)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T01:59:06.270419Z","iopub.execute_input":"2025-01-04T01:59:06.270710Z","iopub.status.idle":"2025-01-04T01:59:10.551354Z","shell.execute_reply.started":"2025-01-04T01:59:06.270689Z","shell.execute_reply":"2025-01-04T01:59:10.550418Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Displaying a sample of the Model's Predictions","metadata":{}},{"cell_type":"code","source":"# Randomly select 10 indices from X_test\nrandom_indices = random.sample(range(len(X_test)), 10)\n\n# Iterate over the selected indices\nfor idx in random_indices:\n    \n    sample_text = X_test[idx]  # Get the original text sample\n    result = predictions[idx]    # Get the model prediction\n    y_pred_classes = np.argmax(result)\n\n    label = \"positive\" if y_pred_classes == 1 else \"negative\" \n    \n    # Print the sample and its prediction\n    print(\"Sample:\", sample_text, \" ---> \", label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T01:59:10.552426Z","iopub.execute_input":"2025-01-04T01:59:10.552702Z","iopub.status.idle":"2025-01-04T01:59:10.563292Z","shell.execute_reply.started":"2025-01-04T01:59:10.552680Z","shell.execute_reply":"2025-01-04T01:59:10.559391Z"}},"outputs":[{"name":"stdout","text":"Sample: on the balcony at kensingtons in the sun   --->  positive\nSample: <user> i ll suffer alongside you   --->  positive\nSample: late lunch then off to cheers for bday drinks   --->  positive\nSample: <user>  why not    --->  negative\nSample: <user> i just need to know what mine is   --->  positive\nSample: <user> morning   happy sunday      --->  positive\nSample: i need to find my disc so i can update my spyware stuff and anti virus stuff    --->  negative\nSample: ughh my dad is on my case   --->  negative\nSample: <user>  i m obviously not meant to send this email out  as its crashed again  and i was so close to the send button   --->  negative\nSample: i am this close to getting that second job  i just have to wait a week   --->  negative\n","output_type":"stream"}],"execution_count":15}]}